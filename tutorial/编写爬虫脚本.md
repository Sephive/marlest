### 编写爬虫脚本
接下来，我们来做个轻松，有意思的东西，爬虫！爬取一些网站上的资讯给我们填充数据。
为了简单，我们的爬虫不需要模拟登录，不需要识别验证码，我不这么做，不代表Scrapy无法做到这几点。下次我会分享几篇Scrapy的高级用法。

此文章不介绍Scrapy的基本用法，所以可以移步[官网](http://doc.scrapy.org/en/latest/)入个门，官网教程还是比较通俗易懂的！

Scrapy的结构和Django有点像

### 编写DOM规则 ###

安装好Scrapy，直接执行命令:

	$ scrapy startproject marlboro
	
然后在 marlboro/marlboro/spiders/ 下新建一个info_crawl.py:
	
	# -*- coding:UTF-8 -*-
    from scrapy.spider import Spider 
    from scrapy.selector import Selector

    class Fx678(Spider):
        name = u'fx'
        allowed_domains = ['www.fx678.com']
        start_urls = ['http://www.fx678.com/news/flash/default.shtml' ]

        def parse(self, response):
            sel = Selector(response)
            infos = sel.xpath('//div[@class="list_content01_title"]')
            for i in infos:
                href = i.xpath('div[1]/a/@href').extract()[0].strip()
                title = i.xpath('div[1]/a/text()').extract()[0].strip()[1:]
                add_time = i.xpath('div[2]/text()').extract()[0].strip()

                site = '' # site作为一个unique的键存在数据库中
                if href.endswith('html') and not href.endswith('index.html'):
                    site = 'fx678_%s' % href.split('/')[-1].split('.')[0]
                

                print title, add_time, site	
       
执行以下命令查看和运行爬虫：
	
	$ scrapy list
	fx
	$ scrapy crawl fx
	……
	……
	奥斯本：未来五年将创造150万个工作岗位 03/19 20:42 fx678_201403192042561484
	奥斯本：乌克兰紧张局势升级或使经济增速下降、通胀率上升 03/19 20:42 fx678_201403192042471484
	奥斯本：今年晚些时候经济将超过危机前的峰值 03/19 20:42 fx678_201403192042381484
	英国责任预算办公室(OBR)12月时预期2016年经济增长2.6%，2017年增长2.7% 03/19 20:41 fx678_201403192041441483
	……
	
可以看到 数据已经成功爬取。不过时间的格式不是我们想要的，我们转换成MySQL可以接收的datetime格式：

	time_string = '%s/%s' % (datetime.date.today().year, add_time)
	time_struct = time.strptime(time_string, '%Y/%m/%d %H:%M')
	dt = datetime.datetime.fromtimestamp(time.mktime(time_struct))

接下来，让我们回顾下我们的需求，我们需要爬虫每几秒种爬一次数据，然后判断**是否有新数据产生**。如果有，就写入MySQL，没有的话就休眠。如果每次判断是否有内容就检索数据库，对数据库的压力太大。我们可以用Redis内存数据帮助我们解决这个问题：我们取两个数据库：一个数据库存放已经爬取的ID；另外一个数据库存放需要我们存入到数据库的数据。

我们在代码

我们在刚才的代码里稍做修改：

	# for 循环块前面加上redis连接语句
	r9 = redis.Redis(host='localhost', port=6379, db=9)
	r8 = redis.Redis(host='localhost', port=6379, db=8)

	# for 循环快末尾插入以下代码

	if site and title and dt:
        sql = u"insert into news_info(title, site, add_time, is_breaking_news, is_hidden) value('%s', '%s', '%s', 0, 0)" % (title, site, dt)
        sen = '{"sql":"%s", "title":"%s", "add_time":"%s", "is_breaking_news":"%s" }' % (sql, title, dt, 0)
        if not r9.exists(site):
            r9.set(site, sen)
            r8.set(site, sen)
        else: r9.set(site, sen)
    else :
        pass  # 可能DOM结构引发爬虫错误，发送邮件通知作者
        
       
### 利用Redis打个辅助 ###
       
假设后期我们还需要写新的爬虫进行其他网站的爬虫，有写过爬虫的朋友知道，如果用一个进程运行容易导致一个爬虫出现问题会导致整个爬虫程序歇菜，所以，我们写个脚本，可以用于分发子进程运行爬虫，且可以便利redis查找可以写入数据库的数据,在scrapy.cfg同目录下新建一个 crawl.py：
	
	#-*- coding:UTF8 -*-
    from mysql import MysqlDriver

    import subprocess
    import time, json, redis

    if __name__ == '__main__':
        r8 = redis.Redis(host='localhost', port=6379, db=8)
        r9 = redis.Redis(host='localhost', port=6379, db=9)
        r8.flushdb()
        r9.flushdb()
        while(True):
            r8.flushdb()
            subprocess.Popen(['scrapy', 'crawl', 'fx'], stdout=subprocess.PIPE).communicate()[0]
            time.sleep(5)
            for i in sorted(r8.keys()):
                s = json.loads(r8.get(i))
                result = MysqlDriver.getInstance().execute('%s' % s['sql'], commit=True)
                if result == 1:
                    MysqlDriver.getInstance().getCursor().execute("select LAST_INSERT_ID()")
                    MysqlDriver.getInstance().getCursor().fetchone()[0]

            time.sleep(600) #仅为了学习之用，我们把休眠时间设置为10分钟，减轻服务器压力
            
关于from mysql import MysqlDriver可以查看Github里的代码。 

### Redis连接池和订阅/发布 ###

以上的代码我们还可以再优化下：

1、Redis的链接用连接池
2、添加到数据库里有，我们往redis数据库发布(publish)一条消息，方便前台订阅信息后推送。

	POOL8 = redis.ConnectionPool(host='localhost', port=6379, db=8)
    POOL9 = redis.ConnectionPool(host='localhost', port=6379, db=9)
    r8 = redis.Redis(connection_pool=POOL8)
    r9 = redis.Redis(connection_pool=POOL9)
    
    …………
    …………
    
    if result == 1:
		MysqlDriver.getInstance().getCursor().execute("select LAST_INSERT_ID()")
		id = MysqlDriver.getInstance().getCursor().fetchone()[0]
		r_s = redis.StrictRedis(host='localhost', port=6379)
		r_s.publish('infos', ' {"id":%s, "title":"%s", "is_breaking_news":"%s" , "add_time":"%s" }' % (id, s['title'], s['is_breaking_news'], s['add_time']) )


运行爬虫就简单了:
	
	$ nohup python crawl.py >> /data/log/crawl.log & 
	
nohup命令在你退出终端后可以被挂载到1号进程上，而不会随着你的终端关闭而关闭。 最后一个 & 是让你的程序后台运行。